# Control vs Discernment in AI Systems

<!-- This is a SAMPLE writing sample. Replace with your own writing. -->
<!-- Include different types of writing so Claude can see your range. -->

Think about the difference between a passenger plane and a fighter jet.

A passenger plane is designed for stability. If you let go of the controls,
it keeps flying straight. The system defaults to safe. That's **control** —
you constrain the system so that bad outcomes are hard to reach.

A fighter jet is the opposite. It's inherently unstable — that's what makes
it maneuverable. The pilot needs real-time judgment to keep it in the air.
That's **discernment** — the system relies on the operator's ability to make
good decisions under uncertainty.

Most AI safety work focuses on control. Alignment, RLHF, constitutional AI —
these are all ways to make the system default to safe. And that's important.

But as models get more capable and autonomous, we increasingly need
discernment too. We need models (and the people deploying them) to exercise
judgment about when to act, when to defer, and when to flag uncertainty.

The question isn't control OR discernment. It's: what's the right mix at
each capability level?
